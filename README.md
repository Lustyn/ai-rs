# AI SDK for Rust

A modular, type-safe Rust SDK for building AI-powered applications with support for multiple providers, tool calling, and agent workflows.

## ‚ú® Features

- **üîí Type Safety**: Compile-time guarantees for messages, tools, and responses
- **üß© Modular Design**: Use only the components you need
- **‚ö° Async/Await**: Built on tokio with full async support
- **üõ†Ô∏è Tool Calling**: Type-safe function calling with automatic JSON schema generation
- **ü§ñ Agent Framework**: High-level orchestration with configurable execution strategies
- **üì° Streaming**: Real-time response streaming
- **üë• Human-in-the-Loop**: Support for client-side tool execution
- **üîÑ Multi-Provider**: Unified interface across different AI providers

## üì¶ Architecture

The SDK is organized into focused crates:

### `ai-core`
Core types and abstractions used by all other components:
- Message types and conversation handling
- Provider traits for different AI capabilities
- Type-safe tool system with schema generation
- Comprehensive error handling

### `ai-anthropic` 
Anthropic Claude API implementation:
- Claude Sonnet, Haiku, and Opus support
- Streaming and non-streaming generation
- Tool calling and vision capabilities
- Rate limiting and error handling

### `ai-agent`
High-level agent execution framework:
- Configurable termination strategies
- Automatic tool calling orchestration
- Multi-step conversation management
- Streaming agent execution

## üöÄ Quick Start

### Installation

Add the crates you need to your `Cargo.toml`:

```toml
[dependencies]
# Core types and traits (always needed)
ai-core = { path = "crates/core" }
# Choose your providers
ai-anthropic = { path = "crates/anthropic" }
# Agent framework (optional, for high-level orchestration)  
ai-agent = { path = "crates/agent" }

# Supporting libraries
tokio = { version = "1.0", features = ["full"] }
schemars = "1.0"  # For tool schema generation
serde = { version = "1.0", features = ["derive"] }
```

### Basic Provider Usage

```rust
use ai_core::*;
use ai_anthropic::*;

#[tokio::main]
async fn main() -> Result<()> {
    // Create provider configuration
    let api_key = std::env::var("ANTHROPIC_API_KEY").expect("API key required");
    let config = AnthropicConfig::new(api_key, "claude-3-5-sonnet");
    let provider = AnthropicProvider::new(config)?;

    // Simple chat request
    let request = ChatRequest::new()
        .system("You are a helpful AI assistant.")
        .user("What is the capital of France?")
        .temperature(0.7);

    let response = provider.generate(request).await?;
    println!("Response: {:?}", response);

    Ok(())
}
```

### Agent Framework Usage

```rust
use ai_core::*;
use ai_anthropic::*;
use ai_agent::*;

// Using the high-level agent framework
let config = GenerateConfig::new(provider)
    .messages(vec![
        Message::system("You are a helpful AI assistant."),
        Message::user("What is the capital of France?")
    ])
    .max_tokens(1000)
    .temperature(0.7)
    .run_until(MaxSteps::new(3));

// Generate with agent orchestration
let response = generate_text(config).await?;
println!("Agent completed in {} steps", response.steps);
```

## üõ†Ô∏è Tool Calling

Create type-safe tools with automatic schema generation:

```rust
use ai_core::{*, errors::{ToolExecutionError, ToolResult}};
use ai_anthropic::*;
use ai_agent::*;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};

#[derive(Clone)]
struct AppState {
    calculator_history: Vec<String>,
}

#[derive(Deserialize, JsonSchema)]
struct CalculatorInput {
    expression: String,
}

// Tool handler that can return errors
fn calculator(
    State(mut state): State<AppState>, 
    input: CalculatorInput
) -> ToolResult<serde_json::Value> {
    // Your calculation logic here
    let result = 42.0; // Placeholder
    
    state.calculator_history.push(format!("{} = {}", input.expression, result));
    
    Ok(serde_json::json!({
        "result": result,
        "expression": input.expression
    }))
}

#[tokio::main]
async fn main() -> Result<()> {
    let config = AnthropicConfig::new("api-key", "claude-3-5-sonnet");
    let provider = AnthropicProvider::new(config)?;
    
    // Create tool router
    let router = ToolRouter::new()
        .register("calculator", Some("Perform calculations".to_string()), calculator)
        .with_state(AppState { calculator_history: Vec::new() });
    
    // Use with agent
    let config = GenerateConfig::new(provider)
        .messages(vec![
            Message::system("You are a calculator assistant."),
            Message::user("What's 15 * 23?"),
        ])
        .tools(router)
        .run_until(MaxSteps::new(5));
        
    let response = generate_text(config).await?;
    println!("Agent completed in {} steps", response.steps);
    
    Ok(())
}
```

## ü§ñ Agent Execution Strategies

Configure how agents should terminate:

```rust
use ai_agent::*;

// Stop after maximum steps
let config = GenerateConfig::new(provider)
    .run_until(MaxSteps::new(5));

// Stop on specific finish reasons
let config = GenerateConfig::new(provider)
    .run_until(StopOnReason::stop_on_finish());

// Combine strategies
let combined = RunUntilFirst::new(
    MaxSteps::new(10),
    StopOnReason::stop_on_finish()
);
let config = GenerateConfig::new(provider).run_until(combined);
```

## üì¶ Project Structure

```
ai-rs/
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ core/          # Core types, traits, and tools system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.rs  # Comprehensive error handling  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.rs   # Message types, requests/responses
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ provider.rs # Provider traits
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools.rs   # Type-safe tool system
‚îÇ   ‚îú‚îÄ‚îÄ anthropic/     # Anthropic Claude implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ provider.rs
‚îÇ   ‚îî‚îÄ‚îÄ agent/         # High-level agent orchestration
‚îÇ       ‚îî‚îÄ‚îÄ agent.rs
‚îú‚îÄ‚îÄ examples/          # Comprehensive examples
‚îî‚îÄ‚îÄ Cargo.toml         # Workspace configuration
```

## üîß Supported Providers

### Anthropic Claude

- ‚úÖ Text generation (Claude Sonnet, Haiku, Opus)
- ‚úÖ Streaming responses
- ‚úÖ Tool calling
- ‚úÖ Vision capabilities
- ‚úÖ System messages
- ‚úÖ Agent framework integration

### Coming Soon

- **OpenAI**: GPT-4, GPT-3.5, tool calling, vision, embeddings
- **Local Models**: Ollama integration, custom model support
- **Google**: Gemini support

## üìö Examples

Run the included examples to see the SDK in action:

```bash
# Set your API key
export ANTHROPIC_API_KEY="your-api-key-here"

# Run all examples
cargo run --bin agents

# Run specific examples
cargo run --bin provider_usage  # Basic provider usage
cargo run --bin mixed_tools     # Tool system demo
cargo run --bin agents         # Agent framework demo
```

### Example Scenarios

- **provider_usage.rs**: Basic provider usage without agents
- **mixed_tools.rs**: Tool system with fallible and infallible tools
- **agents/**: Advanced agent examples with tool calling and HITL scenarios

## üß™ Development

### Running Tests

```bash
# Check all crates
cargo check --workspace

# Run unit tests
cargo test --workspace

# Build release
cargo build --workspace --release
```

### Running Integration Tests

Integration tests require API keys and are marked with `#[ignore]` to avoid hitting APIs during regular test runs.

1. **Set up environment**:
   ```bash
   # Copy the example environment file
   cp .env.example .env
   
   # Add your API keys to .env
   echo "ANTHROPIC_API_KEY=your_actual_key_here" >> .env
   ```

2. **Run integration tests**:
   ```bash
   # Run all integration tests (requires API keys)
   cargo test --package ai-anthropic -- --ignored
   
   # Run specific integration test
   cargo test --package ai-anthropic test_basic_conversation -- --ignored
   
   # Run with output
   cargo test --package ai-anthropic -- --ignored --nocapture
   ```

3. **Available Integration Tests**:
   - `test_basic_conversation` - Simple text generation
   - `test_streaming_conversation` - Streaming responses
   - `test_conversation_with_system_message` - System message handling
   - `test_tool_use_conversation` - Tool calling functionality  
   - `test_multi_turn_conversation` - Multi-turn context preservation
   - `test_image_conversation` - Vision capabilities (Claude 3.5+)
   - `test_error_handling` - Authentication and error scenarios
   - `test_provider_capabilities` - Provider metadata and features
   - `test_conversation_builder_pattern` - Request builder pattern

**‚ö†Ô∏è Note**: Integration tests make real API calls and will consume tokens from your account. Use responsibly.

## üöß Roadmap

- **Enhanced Tool System**: Tool composition, chaining, and async tools
- **More Providers**: OpenAI, Google, local models via Ollama
- **Advanced Features**: Embeddings, image generation, voice synthesis
- **Performance**: Caching, rate limiting, connection pooling
- **Integration**: Web frameworks, CLI tools, desktop apps

## ü§ù Contributing

Contributions welcome! Please see our contributing guidelines for details on:

- Code style and testing requirements
- Adding new providers
- Extending the tool system
- Documentation improvements

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.